{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "719cca0a-e987-4303-9bd7-fd2e9df6156d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, psycopg2, folium, pandas as pd, numpy as np\n",
    "from sqlalchemy import create_engine\n",
    "import matplotlib.pyplot as plt, matplotlib.colors as mcolors\n",
    "az.style.use(\"arviz-whitegrid\")\n",
    "plt.rcParams[\"figure.figsize\"] = [20, 6]\n",
    "plt.rcParams[\"figure.dpi\"] = 100\n",
    "plt.rcParams[\"xtick.labelsize\"] = 20\n",
    "plt.rcParams[\"ytick.labelsize\"] = 20\n",
    "plt.rcParams[\"axes.labelsize\"] = 20\n",
    "plt.rcParams[\"legend.fontsize\"] = 20\n",
    "import warnings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eee122d-4aae-43ba-991c-226f441ddecf",
   "metadata": {},
   "source": [
    "# db connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c8cd59-9d92-49fa-b553-c45e68a8202f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine \n",
    "from sqlalchemy.orm import sessionmaker\n",
    "# database credentials\n",
    "db_usr, db_pwd = '', '' # your database user name and password\n",
    "\n",
    "# database login\n",
    "host, port, db = 'nc-health-data-prod.cluster-ccsgl7rk4urn.eu-central-1.rds.amazonaws.com', 5432, 'master'\n",
    "engine = create_engine('postgresql://'+db_usr+':'+db_pwd+'@'+host+':'+str(port)+'/'+db)\n",
    "Session = sessionmaker(bind=engine)\n",
    "session = Session()\n",
    "conn = engine.connect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edbdd3b7-a176-4e25-a048-41ffa69d257a",
   "metadata": {},
   "source": [
    "# data\n",
    "\n",
    "## activity before and after mass event location visit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71256930-2008-4824-a142-5cc1d9b96661",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "    select ex.\"day\", ex.did, sum(array_length(stime_arr, 1)) as nping\n",
    "    from euro_stuttgart as raf\n",
    "    join ex_corona_sdkv6_2024_27 as ex\n",
    "    on ex.\"day\" = raf.\"day\" and ex.did = raf.did\n",
    "    group by 1, 2\n",
    "\"\"\"\n",
    "data_pingcnt = pd.DataFrame(pd.read_sql_query(query, conn))\n",
    "data_pingcnt = data_pingcnt.sort_values('nping', ascending=False)\n",
    "data_pingcnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad4f6f4-88a2-4fa5-9818-5180d4635dd5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "WITH t1 AS (\n",
    "    SELECT \n",
    "        ex.\"day\", \n",
    "        ex.did, \n",
    "        SUM(array_length(stime_arr, 1)) AS nping\n",
    "    FROM euro_stuttgart AS raf\n",
    "    JOIN (\n",
    "        SELECT * FROM ex_corona_sdkv6_2024_25\n",
    "        UNION ALL\n",
    "        SELECT * FROM ex_corona_sdkv6_2024_27\n",
    "    ) AS ex \n",
    "        ON ex.\"day\" = raf.\"day\" AND ex.did = raf.did\n",
    "    GROUP BY ex.\"day\", ex.did\n",
    "),\n",
    "pings AS (\n",
    "    SELECT\n",
    "        ex.\"day\",\n",
    "        ex.did,\n",
    "        ex.tile_id,\n",
    "        UNNEST(stime_arr) AS stime,\n",
    "        UNNEST(tile_arr) AS tl8\n",
    "    FROM (\n",
    "        SELECT * FROM ex_corona_sdkv6_2024_25\n",
    "        UNION ALL\n",
    "        SELECT * FROM ex_corona_sdkv6_2024_27\n",
    "    ) AS ex\n",
    "    JOIN t1 \n",
    "        ON t1.did = ex.did AND t1.\"day\" = ex.\"day\"\n",
    "),\n",
    "traj AS (\n",
    "    SELECT\n",
    "        p.\"day\",\n",
    "        p.did,\n",
    "        p.stime,\n",
    "        p.tl8,\n",
    "        ST_Transform(\n",
    "            ST_Translate(\n",
    "                ST_SetSRID(tile8togeo(p.tl8), 32632),\n",
    "                tx.minx,\n",
    "                tx.miny\n",
    "            ),\n",
    "            3857\n",
    "        ) AS geopoint\n",
    "    FROM pings AS p\n",
    "    JOIN txc_dt_grid_1000m AS tx \n",
    "        ON p.tile_id = tx.tile_id\n",
    ")\n",
    "SELECT\n",
    "    \"day\",\n",
    "    did,\n",
    "    stime,\n",
    "    ST_X(ST_Transform(geopoint, 4326)) AS ping_lon,\n",
    "    ST_Y(ST_Transform(geopoint, 4326)) AS ping_lat\n",
    "FROM traj\n",
    "\"\"\"\n",
    "\n",
    "data_traj = pd.DataFrame(pd.read_sql_query(query, conn))\n",
    "data_traj['ping_lon_next'] = data_traj.ping_lon.shift(-1)\n",
    "data_traj['ping_lat_next'] = data_traj.ping_lat.shift(-1)\n",
    "data_traj\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f423ef5-e433-4fca-b55f-9db4021989c5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_traj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "191a10ac-f5cf-4345-9bb5-0a0407b43e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_traj.to_csv('Stuttgart_dids_match_pings.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8eaae56-6de1-426a-8ddd-1cd6319f33d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_traj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d6ea44-5b97-4aa1-a688-5e7352057a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_traj['stime'] = pd.to_datetime(data_traj['stime'])\n",
    "data_traj['day'] = pd.to_datetime(data_traj['day'])\n",
    "data_traj['did'] = data_traj['did'].astype(str) + data_traj['day'].astype(str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78323835-478e-4131-bcd7-67dee9648568",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_traj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ff6633-d55d-4f69-b418-90c17fce3913",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ensure stime is a datetime\n",
    "# sort by did and stime\n",
    "data_traj = data_traj.sort_values(['did', 'stime'])\n",
    "\n",
    "# downsample: one ping every minute per did\n",
    "data_traj_1min = (\n",
    "    data_traj\n",
    "    .groupby('did')\n",
    "    .apply(lambda df: df.set_index('stime').resample('1min').first())\n",
    "    .dropna(subset=['ping_lon', 'ping_lat'])\n",
    "    .reset_index(level=0, drop=True)\n",
    "    .reset_index()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e82f2c9-08e0-4273-ad99-3e3fbaddf020",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_traj_1min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04573b6b-2a4b-4c41-98f7-a47081e929f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_traj_5min = (\n",
    "    data_traj\n",
    "    .groupby('did')\n",
    "    .apply(lambda df: df.set_index('stime').resample('5min').first())\n",
    "    .dropna(subset=['ping_lon', 'ping_lat'])  # optional: keep only valid pings\n",
    "    .reset_index(level=0, drop=True)\n",
    "    .reset_index()\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab6d5601-556c-44e7-931f-e1cf2e836eec",
   "metadata": {},
   "source": [
    "## Maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b22a57-a233-4624-9a1a-5fc1d255891d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from folium import Map, PolyLine\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import colors as mcolors\n",
    "from folium.plugins import FloatImage\n",
    "import datetime\n",
    "\n",
    "import base64\n",
    "from folium.plugins import FloatImage\n",
    "\n",
    "# map centered at Stuttgart\n",
    "m = Map(location=[48.7758, 9.1829], zoom_start=8)\n",
    "\n",
    "\n",
    "# Normalize using just the hour (0 to 24)\n",
    "vmin = 0\n",
    "vmax = 24\n",
    "norm = mcolors.Normalize(vmin=vmin, vmax=vmax)\n",
    "cmap = plt.get_cmap('gist_rainbow')\n",
    "\n",
    "# Assuming 'data_traj_1min' is already sorted by did and stime.\n",
    "for idx, (did, df) in enumerate(data_traj.sort_values(['did', 'stime']).groupby('did')):\n",
    "    df = df.reset_index(drop=True)\n",
    "    for i in range(len(df) - 1):\n",
    "        lat1, lon1 = df.loc[i, ['ping_lat', 'ping_lon']]\n",
    "        lat2, lon2 = df.loc[i + 1, ['ping_lat', 'ping_lon']]\n",
    "        stime = df.loc[i, 'stime']\n",
    "        # Compute the hour value as a float (e.g., 16:30 becomes 16.5)\n",
    "        hour_value = stime.hour \n",
    "        color = mcolors.to_hex(cmap(norm(hour_value)))\n",
    "        folium.PolyLine(\n",
    "            [[lat1, lon1], [lat2, lon2]],\n",
    "            color=color,\n",
    "            weight=4,\n",
    "            opacity=0.7\n",
    "        ).add_to(m)\n",
    "\n",
    "\n",
    "# Create and save the colorbar as an image\n",
    "fig, ax = plt.subplots(figsize=(6, 0.5))\n",
    "fig.subplots_adjust(bottom=0.5)\n",
    "sm = plt.cm.ScalarMappable(cmap=cmap, norm=norm)\n",
    "cb = plt.colorbar(sm, cax=ax, orientation='horizontal')\n",
    "# Format ticks as \"HH:00\"\n",
    "tick_locs = cb.get_ticks()\n",
    "tick_labels = [ts for ts in tick_locs]\n",
    "cb.set_ticks(tick_locs)\n",
    "cb.set_ticklabels(tick_labels)\n",
    "plt.title(\"Hour â†’ Color Mapping\")\n",
    "plt.savefig(\"colorbar.png\", bbox_inches='tight', dpi=150)\n",
    "plt.close()\n",
    "\n",
    "with open(\"colorbar.png\", \"rb\") as f:\n",
    "    encoded = base64.b64encode(f.read()).decode('utf-8')\n",
    "data_uri = f\"data:image/png;base64,{encoded}\"\n",
    "\n",
    "# Add the colorbar image as a floating legend to the folium map\n",
    "FloatImage(data_uri, bottom=5, left=10).add_to(m)\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "277f1493-8e58-41ba-867e-da9613efbec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "from folium import Map, PolyLine, CircleMarker\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import colors as mcolors\n",
    "from folium.plugins import FloatImage\n",
    "import datetime\n",
    "import folium\n",
    "\n",
    "# Map centered at Stuttgart\n",
    "m = Map(location=[48.7758, 9.1829], zoom_start=10)\n",
    "\n",
    "# Filter to just one device\n",
    "did_target = \"F608E0B9121E5A3518814E0952082E3A2024-06-19\"\n",
    "df_did = data_traj_1min[data_traj_1min.did == did_target].sort_values('stime').reset_index(drop=True)\n",
    "df_did_5min = data_traj_5min[data_traj_5min.did == did_target].sort_values('stime').reset_index(drop=True)\n",
    "\n",
    "# Normalize using just the hour (0 to 24)\n",
    "vmin = 0\n",
    "vmax = 24\n",
    "norm = mcolors.Normalize(vmin=vmin, vmax=vmax)\n",
    "cmap = plt.get_cmap('gist_rainbow')\n",
    "\n",
    "# Draw colored segments based on hour\n",
    "for i in range(len(df_did) - 1):\n",
    "    lat1, lon1 = df_did.loc[i, ['ping_lat', 'ping_lon']]\n",
    "    lat2, lon2 = df_did.loc[i + 1, ['ping_lat', 'ping_lon']]\n",
    "    stime = df_did.loc[i, 'stime']\n",
    "    # Use hour value (you could also add minute/60 for more precision)\n",
    "    hour_value = stime.hour  \n",
    "    color = mcolors.to_hex(cmap(norm(hour_value)))\n",
    "    PolyLine(\n",
    "        [[lat1, lon1], [lat2, lon2]],\n",
    "        color=color,\n",
    "        weight=4,\n",
    "        opacity=0.7\n",
    "    ).add_to(m)\n",
    "\n",
    "# Process the 5-minute data for static black points\n",
    "for lat, lon in zip(df_did_5min.ping_lat, df_did_5min.ping_lon):\n",
    "    CircleMarker(\n",
    "        location=[lat, lon],\n",
    "        radius=1,\n",
    "        color='black',\n",
    "        fill=True,\n",
    "        fill_color='black',\n",
    "        fill_opacity=0.5\n",
    "    ).add_to(m)\n",
    "\n",
    "# -------------------------\n",
    "\n",
    "# -------------------------\n",
    "# Convert the colorbar image to base64 for embedding in the map\n",
    "with open(\"colorbar.png\", \"rb\") as f:\n",
    "    encoded = base64.b64encode(f.read()).decode('utf-8')\n",
    "data_uri = f\"data:image/png;base64,{encoded}\"\n",
    "\n",
    "# Add the colorbar image as a floating legend to the folium map\n",
    "FloatImage(data_uri, bottom=5, left=10).add_to(m)\n",
    "\n",
    "m\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef1967a7-cab2-4bda-af61-59aa521032d5",
   "metadata": {},
   "source": [
    "## Heatmap per hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f852521-ffa1-4adf-86a0-eb45a11dce64",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import folium\n",
    "from folium.plugins import HeatMap\n",
    "import pandas as pd\n",
    "\n",
    "# convert stime to datetime, if it's not already\n",
    "data_traj['stime'] = pd.to_datetime(data_traj['stime'])\n",
    "\n",
    "# extract hour from stime\n",
    "data_traj['hour'] = data_traj['stime'].dt.hour\n",
    "\n",
    "# filter for the hour you want\n",
    "data_hour = data_traj[data_traj['hour'] == 19]\n",
    "\n",
    "# define a center location and zoom for your map (modify as needed)\n",
    "m = folium.Map(location=[48.7758, 9.1829], zoom_start=6)\n",
    "\n",
    "# build the heat map using ping latitude and longitude\n",
    "heat_data = data_hour[['ping_lat', 'ping_lon']].values.tolist()\n",
    "HeatMap(heat_data, name='hour', radius=6, blur=5).add_to(m)\n",
    "\n",
    "m\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7edc12c9-afc4-43dc-be83-a37f67e9a19b",
   "metadata": {},
   "source": [
    "For the heatmaps we use the complete data of the two matches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7927a64f-187d-4d5b-bc62-db2cac182684",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import folium\n",
    "from folium.plugins import HeatMap\n",
    "import pandas as pd\n",
    "import os\n",
    "# ensure stime is datetime\n",
    "data_traj['stime'] = pd.to_datetime(data_traj['stime'])\n",
    "\n",
    "# extract hour\n",
    "data_traj['hour'] = data_traj['stime'].dt.hour\n",
    "\n",
    "\n",
    "# get unique hours in sorted order\n",
    "hours_sorted = sorted(data_traj['hour'].unique())\n",
    "\n",
    "# create output directory to store the html files\n",
    "html_dir = 'hourly_html_maps'\n",
    "os.makedirs(html_dir, exist_ok=True)\n",
    "\n",
    "def create_heat_map_for_hour(hour_val, data):\n",
    "    # filter data for the given hour\n",
    "    df_hour = data[data['hour'] == hour_val]\n",
    "    \n",
    "    # create folium map\n",
    "    m = folium.Map(location=[48.7758, 9.1829], zoom_start=13)\n",
    "    heat_data = df_hour[['ping_lat', 'ping_lon']].values.tolist()\n",
    "    \n",
    "    # add heat map layer\n",
    "    HeatMap(heat_data, radius=6, blur=5).add_to(m)\n",
    "    return m\n",
    "\n",
    "# generate one html map per hour\n",
    "for hr in hours_sorted:\n",
    "    m_hour = create_heat_map_for_hour(hr, data_traj)\n",
    "    outfile = os.path.join(html_dir, f'heatmap_hour_{hr}.html')\n",
    "    m_hour.save(outfile)\n",
    "    print(f'Saved: {outfile}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bebabb16-f4c4-4130-a6d0-3b85e62e0e33",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "import time\n",
    "import os\n",
    "import re\n",
    "\n",
    "# Function to extract hour from filenames like \"heatmap_hour_13.html\"\n",
    "def extract_hour(filename):\n",
    "    match = re.search(r'heatmap_hour_(\\d+).html', filename)\n",
    "    return int(match.group(1)) if match else -1\n",
    "\n",
    "# Use Firefox; make sure geckodriver is in your PATH\n",
    "driver = webdriver.Firefox()\n",
    "\n",
    "html_files = sorted([f for f in os.listdir(html_dir) if f.endswith('.html')],\n",
    "                    key=extract_hour)\n",
    "\n",
    "png_dir = 'hourly_png_maps'\n",
    "os.makedirs(png_dir, exist_ok=True)\n",
    "\n",
    "for html_file in html_files:\n",
    "    file_path = os.path.join(html_dir, html_file)\n",
    "    driver.get('file://' + os.path.abspath(file_path))\n",
    "    \n",
    "    # Allow the map to load\n",
    "    time.sleep(2)\n",
    "    \n",
    "    driver.set_window_size(1200, 900)\n",
    "    \n",
    "    png_file = os.path.join(png_dir, html_file.replace('.html', '.png'))\n",
    "    driver.save_screenshot(png_file)\n",
    "    print(f'Saved screenshot: {png_file}')\n",
    "\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fcaee24-aeed-4968-9842-add1b6b2f4cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import imageio\n",
    "\n",
    "# Directories\n",
    "input_dir = 'hourly_png_maps'         # Folder with your original PNG maps\n",
    "annotated_dir = 'hourly_png_annotated'  # Folder to save annotated PNG images\n",
    "os.makedirs(annotated_dir, exist_ok=True)\n",
    "\n",
    "def extract_hour(filename):\n",
    "    \"\"\"\n",
    "    Extracts the hour as an integer from a filename formatted as \"heatmap_hour_{HOUR}.png\".\n",
    "    \"\"\"\n",
    "    match = re.search(r'heatmap_hour_(\\d+)\\.png', filename)\n",
    "    return int(match.group(1)) if match else -1\n",
    "\n",
    "# Get PNG files sorted numerically by hour\n",
    "png_files = sorted([f for f in os.listdir(input_dir) if f.endswith('.png')],\n",
    "                   key=extract_hour)\n",
    "\n",
    "# Annotate each image with the hour\n",
    "for png_file in png_files:\n",
    "    img_path = os.path.join(input_dir, png_file)\n",
    "    img = Image.open(img_path)\n",
    "    draw = ImageDraw.Draw(img)\n",
    "    \n",
    "    # Load a TrueType font; fallback to default if not available\n",
    "    try:\n",
    "        font = ImageFont.truetype(\"arial.ttf\", 40)\n",
    "    except IOError:\n",
    "        font = ImageFont.load_default()\n",
    "    \n",
    "    hour = extract_hour(png_file)\n",
    "    text = f\"Hour: {hour:02d}\"\n",
    "    \n",
    "    # Position for the text (with padding)\n",
    "    position = (10, 10)\n",
    "    \n",
    "    # Optional: add a shadow for better visibility\n",
    "    shadow_color = \"black\"\n",
    "    for offset in [(1, 1), (-1, -1), (1, -1), (-1, 1)]:\n",
    "        pos = (position[0] + offset[0], position[1] + offset[1])\n",
    "        draw.text(pos, text, font=font, fill=shadow_color)\n",
    "    \n",
    "    # Draw the text in white\n",
    "    draw.text(position, text, font=font, fill=\"white\")\n",
    "    \n",
    "    # Save the annotated image\n",
    "    annotated_path = os.path.join(annotated_dir, png_file)\n",
    "    img.save(annotated_path)\n",
    "    print(f\"Annotated and saved: {annotated_path}\")\n",
    "\n",
    "# Assemble annotated images into an animated GIF\n",
    "annotated_files = sorted([f for f in os.listdir(annotated_dir) if f.endswith('.png')],\n",
    "                          key=extract_hour)\n",
    "images = [imageio.imread(os.path.join(annotated_dir, f)) for f in annotated_files]\n",
    "\n",
    "gif_filename = 'density_map_hourly_annotated.gif'\n",
    "imageio.mimsave(gif_filename, images, duration=1.0)  # duration=1.0 sec per frame\n",
    "print(f\"GIF created: {gif_filename}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864da829-f3ea-44ab-be21-05b2ee22515b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "import re\n",
    "\n",
    "def extract_hour(filename):\n",
    "    \"\"\"\n",
    "    Extracts the hour as an integer from a filename like \"heatmap_hour_13.png\"\n",
    "    \"\"\"\n",
    "    match = re.search(r'heatmap_hour_(\\d+)\\.png', filename)\n",
    "    return int(match.group(1)) if match else -1\n",
    "\n",
    "# Directory containing your annotated PNG images\n",
    "annotated_dir = 'hourly_png_annotated'\n",
    "annotated_files = sorted(\n",
    "    [f for f in os.listdir(annotated_dir) if f.endswith('.png')],\n",
    "    key=extract_hour\n",
    ")\n",
    "\n",
    "frames = []\n",
    "for file in annotated_files:\n",
    "    path = os.path.join(annotated_dir, file)\n",
    "    im = Image.open(path)\n",
    "    # Convert image to 'P' mode using an adaptive palette with dithering for smoother colors\n",
    "    im_p = im.convert('P', palette=Image.ADAPTIVE, dither=Image.FLOYDSTEINBERG)\n",
    "    frames.append(im_p)\n",
    "\n",
    "# Save frames as an animated GIF with optimized palette and dithering\n",
    "gif_filename = 'density_map_hourly_annotated_near.gif'\n",
    "frames[0].save(gif_filename, save_all=True, append_images=frames[1:], duration=1000, loop=0, optimize=True)\n",
    "print(f\"Optimized GIF created: {gif_filename}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a43f3371-74a4-4cc1-9329-7be330d3ae14",
   "metadata": {},
   "source": [
    "## Activities before and after (Clustering and Events)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa87b68-3fc0-4f55-96b9-6bfe76e0bf40",
   "metadata": {},
   "source": [
    "## Examples to calibrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68bde065-84b0-431b-bd4d-d30a3e8a8f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.cluster import DBSCAN\n",
    "from scipy.stats import mode\n",
    "\n",
    "# --- Clustering Class Definition ---\n",
    "class ClusteringLocs:\n",
    "    def __init__(self, lon_ar, lat_ar, eps=0.001, min_samples=5):\n",
    "        self.eps = eps\n",
    "        self.min_samples = min_samples\n",
    "        self.array = np.array(list(zip(lon_ar, lat_ar)))\n",
    "        self.db = None\n",
    "        if len(self.array) > 2:\n",
    "            self.db = DBSCAN(eps=self.eps, min_samples=self.min_samples).fit(self.array)\n",
    "    \n",
    "    def labels(self):\n",
    "        if self.db is not None:\n",
    "            return self.db.labels_\n",
    "        else:\n",
    "            return np.array([])\n",
    "        \n",
    "    def num_clus(self):\n",
    "        if self.db is not None:\n",
    "            num_clusters = len(set(self.db.labels_)) - (1 if -1 in self.db.labels_ else 0)\n",
    "            return num_clusters\n",
    "        else:\n",
    "            return 0\n",
    "        \n",
    "    def clus_centroids(self):\n",
    "        if self.db is not None:\n",
    "            labels1 = self.labels()\n",
    "            centroids = []\n",
    "            for label in set(labels1):\n",
    "                if label != -1:  # Exclude noise points\n",
    "                    mask = (labels1 == label)\n",
    "                    cluster_points = self.array[mask]\n",
    "                    # Compute a representative centroid using the mode of the points.\n",
    "                    centroid = list(mode(cluster_points, keepdims=True).mode[0])\n",
    "                    centroids.append(centroid)\n",
    "            centroids = np.array(centroids)\n",
    "            if centroids.ndim == 1:\n",
    "                return centroids, np.array([])\n",
    "            else:\n",
    "                return centroids[:, 0], centroids[:, 1]\n",
    "        else:\n",
    "            return np.array([]), np.array([])\n",
    "\n",
    "# --- Create DataFrame with did and centroids_ar ---\n",
    "# Assume data_traj is your DataFrame with at least the columns 'did', 'ping_lon', and 'ping_lat'\n",
    "results = []\n",
    "for did in data_traj_1min['did'].unique():\n",
    "    df_did = data_traj_1min[data_traj_1min['did'] == did]\n",
    "    lon_ar = df_did['ping_lon'].values\n",
    "    lat_ar = df_did['ping_lat'].values\n",
    "    clusterer = ClusteringLocs(lon_ar, lat_ar, eps=0.001, min_samples=4)\n",
    "    \n",
    "    if clusterer.num_clus() > 0:\n",
    "        cent_x, cent_y = clusterer.clus_centroids()\n",
    "        # Combine the separate x and y arrays into a list of (lon, lat) tuples.\n",
    "        centroids = [(lon, lat) for lon, lat in zip(cent_x, cent_y)]\n",
    "    else:\n",
    "        centroids = []\n",
    "        \n",
    "    results.append({'did': did, 'centroids_ar': centroids})\n",
    "\n",
    "did_centroids_ar = pd.DataFrame(results)\n",
    "print(did_centroids_ar)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a59691a-d456-42df-b614-938a536d1ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "m = Map(location=[48.7758, 9.1829], zoom_start=8)\n",
    "# filter to just one device\n",
    "did_target = \"050CC00BB91751B47DD1E88117830D162024-06-19\"\n",
    "df_did = data_traj_1min[data_traj_1min.did == did_target].sort_values('stime').reset_index(drop=True)\n",
    "df_did_5min = data_traj_5min[data_traj_5min.did == did_target].sort_values('stime').reset_index(drop=True)\n",
    "\n",
    "\n",
    "import folium\n",
    "import matplotlib.colors as mcolors\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import numpy as np\n",
    "from sklearn.cluster import DBSCAN\n",
    "from scipy.stats import mode\n",
    "\n",
    "# -----------------------------\n",
    "# 1. Create the Map\n",
    "# -----------------------------\n",
    "m = folium.Map(location=[48.7758, 9.1829], zoom_start=11)\n",
    "\n",
    "# -----------------------------\n",
    "# 2. Define normalization bounds for the day (midnight to 23:59:59)\n",
    "# -----------------------------\n",
    "vmin = 0\n",
    "vmax = 24\n",
    "norm = mcolors.Normalize(vmin=vmin, vmax=vmax)\n",
    "cmap = plt.get_cmap('gist_rainbow')\n",
    "\n",
    "\n",
    "for i in range(len(df_did) - 1):\n",
    "    lat1, lon1 = df_did.loc[i, ['ping_lat', 'ping_lon']]\n",
    "    lat2, lon2 = df_did.loc[i + 1, ['ping_lat', 'ping_lon']]\n",
    "    stime = df_did.loc[i, 'stime']\n",
    "    hour_value = stime.hour \n",
    "    color = mcolors.to_hex(cmap(norm(hour_value)))\n",
    "    folium.PolyLine(\n",
    "        [[lat1, lon1], [lat2, lon2]],\n",
    "        color=color,\n",
    "        weight=4,\n",
    "        opacity=0.7\n",
    "    ).add_to(m)\n",
    "for lat, lon, stime in zip(df_did_5min.ping_lat,df_did_5min.ping_lon, df_did_5min.stime):\n",
    "    folium.CircleMarker(\n",
    "        location=[lat, lon],\n",
    "         radius=1,\n",
    "            color='black',\n",
    "            fill=True,\n",
    "            fill_color='black',\n",
    "            fill_opacity=0.1,\n",
    "             popup=f\"{did_target} - {stime.strftime('%H:%M:%S')}\"\n",
    "        ).add_to(m)\n",
    "# -----------------------------\n",
    "# 5. Extract clusters for the target device and add circle markers\n",
    "# -----------------------------\n",
    "# Here, we filter the centroids for did_target.\n",
    "clusters = []\n",
    "row = did_centroids_ar[did_centroids_ar['did'] == did_target]\n",
    "if not row.empty:\n",
    "    centroids = row.iloc[0]['centroids_ar']\n",
    "    print(len(centroids))\n",
    "    print(centroids)\n",
    "    for centroid in centroids:\n",
    "        clusters.append(centroid)\n",
    "for centroid in clusters:\n",
    "    folium.CircleMarker(\n",
    "        location=(centroid[1], centroid[0]),  # folium expects [lat, lon]\n",
    "        radius=5,\n",
    "        color='black',\n",
    "        fill=True,\n",
    "        fill_color='black'\n",
    "    ).add_to(m)\n",
    "\n",
    "\n",
    "m  # In a Jupyter notebook, displaying m will show the map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c73de45-d739-450c-b276-481980fc63f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "did_centroids_ar.loc[[5]]['centroids_ar']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a58f755c-d698-4387-bc13-91f4d9467132",
   "metadata": {},
   "source": [
    "## City and Amenities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc8bd1f-9ff5-4acf-866f-614c73cb43ab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def get_osm_amenities(lat, lon, radius=50):\n",
    "    overpass_url = \"http://overpass-api.de/api/interpreter\"\n",
    "    query = f\"\"\"\n",
    "    [out:json];\n",
    "    (\n",
    "      node(around:{radius},{lat},{lon})[amenity];\n",
    "      way(around:{radius},{lat},{lon})[amenity];\n",
    "      relation(around:{radius},{lat},{lon})[amenity];\n",
    "    );\n",
    "    out center;\n",
    "    \"\"\"\n",
    "    response = requests.post(overpass_url, data=query)\n",
    "    data = response.json()\n",
    "    \n",
    "    results = []\n",
    "    for element in data.get('elements', []):\n",
    "        tags = element.get('tags', {})\n",
    "        name = tags.get('name', 'unknown')\n",
    "        amenity_type = tags.get('amenity')\n",
    "        # Try to get the city name from the address if available.\n",
    "        city = tags.get('addr:city')\n",
    "        results.append({'name': name, 'type': amenity_type, 'city': city})\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Apply the OSM lookup to each centroid in your did_centroids_ar DataFrame.\n",
    "osm_info = {}  # Dictionary to hold amenities info per did\n",
    "\n",
    "for idx, row in did_centroids_ar.iterrows():\n",
    "    print(idx)\n",
    "    did = row['did']\n",
    "    centroids = row['centroids_ar']\n",
    "    print(centroids)# List of centroids, each stored as (lon, lat)\n",
    "    amenities_for_centroids = []\n",
    "    \n",
    "    for centroid in centroids:\n",
    "        # Remember: centroids are stored as (lon, lat), but the query expects (lat, lon)\n",
    "        lon, lat = centroid\n",
    "        amenities = get_osm_amenities(lat, lon, radius=50)\n",
    "        amenities_for_centroids.append(amenities)\n",
    "    \n",
    "    osm_info[did] = amenities_for_centroids\n",
    "\n",
    "print(osm_info)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68430e2c-433a-4efb-87d6-98d604be4616",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Helper function to extract amenity types for each centroid.\n",
    "def extract_types(amenities_lists, expected_length):\n",
    "    \"\"\"\n",
    "    Given a list of lists of amenity dictionaries (each with a key 'type'),\n",
    "    returns a list of types with length equal to expected_length.\n",
    "    \n",
    "    For each sublist in amenities_lists:\n",
    "      - If the sublist is empty or no dictionary contains a 'type' key,\n",
    "        append \"unknown\".\n",
    "      - Otherwise, append the type from the first dictionary that has a 'type'.\n",
    "    \n",
    "    If amenities_lists is empty or shorter than expected_length,\n",
    "    the result is padded with \"unknown\" until its length equals expected_length.\n",
    "    \"\"\"\n",
    "    result = []\n",
    "    if not amenities_lists or len(amenities_lists) == 0:\n",
    "        result = [\"unknown\"] * expected_length\n",
    "    else:\n",
    "        for centroid_amenities in amenities_lists:\n",
    "            if not centroid_amenities:\n",
    "                result.append(\"unknown\")\n",
    "            else:\n",
    "                found = False\n",
    "                for amenity in centroid_amenities:\n",
    "                    if 'type' in amenity:\n",
    "                        result.append(amenity['type'])\n",
    "                        found = True\n",
    "                        break\n",
    "                if not found:\n",
    "                    result.append(\"unknown\")\n",
    "    # Pad with \"unknown\" if result is shorter than expected_length.\n",
    "    while len(result) < expected_length:\n",
    "        result.append(\"unknown\")\n",
    "    return result\n",
    "\n",
    "# Now, add a new column 'types' to did_centroids_ar.\n",
    "# We assume that did_centroids_ar has a column 'centroids_ar' (a list of centroids)\n",
    "# and osm_info is a dict mapping did to a list of amenity lists (one per centroid).\n",
    "did_centroids_ar['types'] = did_centroids_ar.apply(\n",
    "    lambda row: extract_types(osm_info.get(row['did'], []), len(row['centroids_ar'])),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "print(did_centroids_ar[['did', 'types']])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70da7a7d-e3e6-4805-999f-177481b5933e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def reverse_geocode(lat, lon):\n",
    "    url = \"https://nominatim.openstreetmap.org/reverse\"\n",
    "    params = {\n",
    "        'lat': lat,\n",
    "        'lon': lon,\n",
    "        'format': 'json',\n",
    "        'addressdetails': 1,\n",
    "        'zoom': 20,  # Higher zoom = more specific (building-level)\n",
    "    }\n",
    "    headers = {\n",
    "        'User-Agent': 'my-geo-app/1.0 (alrinconh@gmail.com)'  # Required by Nominatim usage policy\n",
    "    }\n",
    "\n",
    "    response = requests.get(url, params=params, headers=headers)\n",
    "    data = response.json()\n",
    "\n",
    "    address = data.get(\"address\", {})\n",
    "    display_name = data.get(\"type\")\n",
    "    amenity = data.get(\"class\")\n",
    "    osm_id=data.get(\"osm_id\")\n",
    "    city = address.get(\"city\") or address.get(\"town\") or address.get(\"village\")\n",
    "  \n",
    "   \n",
    "    return {\n",
    "        \"display_name\": display_name,\n",
    "        \"amenity\": amenity,\n",
    "        \"city\": city,\n",
    "        \"osm_id\": osm_id\n",
    "    }\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c13c4f5-f9f9-4950-b4f3-14da8981e9c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_geocode(48.412119250927006,9.066829043028424)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3cd503-6df5-4fa9-8849-69b0f87c1d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary that will map each device id (did) to its list of (city, amenity) pairs\n",
    "nominatim = {}\n",
    "\n",
    "for idx, row in did_centroids_ar.iterrows():\n",
    "    did = row['did']\n",
    "    centroids = row['centroids_ar']  # Expecting a list of centroids stored as (lon, lat) tuples\n",
    "    triples = []\n",
    "    \n",
    "    for centroid in centroids:\n",
    "        # Remember: centroids are stored as (lon, lat) but our function expects (lat, lon)\n",
    "        lon, lat = centroid\n",
    "        result = reverse_geocode(lat, lon)\n",
    "        # Use defaults (\"unknown\") if values are missing\n",
    "        city = result.get(\"city\") if result.get(\"city\") is not None else \"unknown\"\n",
    "        amenity = result.get(\"amenity\") if result.get(\"amenity\") is not None else \"unknown\"\n",
    "        name= result.get(\"display_name\") if result.get(\"display_name\") is not None else \"unknown\"\n",
    "        osm_id= result.get(\"osm_id\") if result.get(\"osm_id\") is not None else \"unknown\"\n",
    "\n",
    "        \n",
    "        triples.append((city, amenity, name, osm_id))\n",
    "    \n",
    "    nominatim[did] = triples\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d1a207-3922-4458-81c6-9f278d777910",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nominatim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04985f13-8140-4175-b43e-8252620061dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the new column \"nominatim_pairs\" to your DataFrame by mapping each did to its corresponding list.\n",
    "did_centroids_ar['nominatim'] = did_centroids_ar['did'].apply(lambda d: nominatim.get(d, []))\n",
    "\n",
    "# Check the result:\n",
    "print(did_centroids_ar[['did', 'nominatim']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d790102e-92bb-4e9b-ab63-330c98d70ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "did_centroids_ar['len'] = did_centroids_ar['centroids_ar'].apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60882e8-5b08-4223-b8f5-4205c17192e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "did_centroids_ar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a83f614-21fb-423a-8816-ec5d271f8237",
   "metadata": {},
   "outputs": [],
   "source": [
    "#did_centroids_ar.to_csv('11_Sttutgart_before_after_german_matches.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ebbe7ba-52e6-4eff-a0d1-32f1f58abe93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter to just one device\n",
    "did_target = \"0D6F298CF70CFF26F8C44C50660009362024-06-19\"\n",
    "df_did = data_traj_1min[data_traj_1min.did == did_target].sort_values('stime').reset_index(drop=True)\n",
    "df_did_5min = data_traj_5min[data_traj_5min.did == did_target].sort_values('stime').reset_index(drop=True)\n",
    "\n",
    "m = folium.Map(location=[48.7758, 9.1829], zoom_start=8)\n",
    "\n",
    "# full-day normalization\n",
    "day = df_did.stime.dt.date.iloc[0]\n",
    "t0 = datetime.datetime.combine(day, datetime.time(0, 0))\n",
    "t1 = datetime.datetime.combine(day, datetime.time(23, 59, 59))\n",
    "vmin = t0.timestamp()\n",
    "vmax = t1.timestamp()\n",
    "norm = mcolors.Normalize(vmin=vmin, vmax=vmax)\n",
    "cmap = plt.get_cmap('gist_rainbow')\n",
    "clusters = []\n",
    "row = did_centroids_ar[did_centroids_ar['did'] == did_target]\n",
    "if not row.empty:\n",
    "    centroids = row.iloc[0]['centroids_ar']\n",
    "    print(len(centroids))\n",
    "    print(centroids)\n",
    "    for centroid in centroids:\n",
    "        clusters.append(centroid)\n",
    "for i in range(len(df_did) - 1):\n",
    "    lat1, lon1 = df_did.loc[i, ['ping_lat', 'ping_lon']]\n",
    "    lat2, lon2 = df_did.loc[i + 1, ['ping_lat', 'ping_lon']]\n",
    "    stime = df_did.loc[i, 'stime']\n",
    "    color = mcolors.to_hex(cmap(norm(stime.timestamp())))\n",
    "    folium.PolyLine(\n",
    "        [[lat1, lon1], [lat2, lon2]],\n",
    "        color=color,\n",
    "        weight=4,\n",
    "        opacity=0.7\n",
    "    ).add_to(m)\n",
    "\n",
    "for centroid in clusters:\n",
    "    folium.CircleMarker(\n",
    "        location=(centroid[1], centroid[0]),  # folium expects [lat, lon]\n",
    "        radius=5,\n",
    "        color='black',\n",
    "        fill=True,\n",
    "        fill_color='black',\n",
    "    ).add_to(m)\n",
    "for lat, lon, stime in zip(df_did_5min.ping_lat,df_did_5min.ping_lon, df_did_5min.stime):\n",
    "    folium.CircleMarker(\n",
    "        location=[lat, lon],\n",
    "         radius=1,\n",
    "            color='black',\n",
    "            fill=True,\n",
    "            fill_color='black',\n",
    "            fill_opacity=0.1,\n",
    "             popup=f\"{did_target} - {stime.strftime('%H:%M:%S')}\"\n",
    "        ).add_to(m)\n",
    "# -----------------------------\n",
    "# 6. Save or display the map\n",
    "# -----------------------------\n",
    "#m.save(\"map.html\")\n",
    "m  # In a Jupyter notebook, displaying m will show the map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40de2bbc-3c4a-45f4-8913-600c6a49a640",
   "metadata": {},
   "outputs": [],
   "source": [
    "did_centroids_ar['nominatim'][4]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbcb428e-f86b-4ea8-8048-764b0678f955",
   "metadata": {},
   "outputs": [],
   "source": [
    "did_centroids_ar['types'][4]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1899f4d2-3c51-457f-bb3b-c77cd68ab42a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to extract all nominatim entries after the stadium target.\n",
    "def extract_after(nom_list):\n",
    "    \"\"\"\n",
    "    Given a list of nominatim entries (tuples), find the last occurrence of the target entry:\n",
    "      ('Stuttgart', 'leisure', 'stadium', 3869991)\n",
    "    and return a list of all entries after it.\n",
    "    If the target is not found, return an empty list.\n",
    "    \"\"\"\n",
    "    target = ('Stuttgart', 'leisure', 'stadium', 3869991)\n",
    "    if not nom_list:\n",
    "        return []\n",
    "    # Find all indices where the target occurs.\n",
    "    indices = [i for i, entry in enumerate(nom_list) if entry == target]\n",
    "    if indices:\n",
    "        last_idx = indices[-1]\n",
    "        return nom_list[last_idx+1:]\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "def extract_origin(nom_list):\n",
    "    \"\"\"\n",
    "    Returns the city (first element) of the first nominatim entry, or None if empty.\n",
    "    \"\"\"\n",
    "    if nom_list and len(nom_list) > 0:\n",
    "        return nom_list[0][0]\n",
    "    return None\n",
    "\n",
    "def extract_final(nom_list):\n",
    "    \"\"\"\n",
    "    Returns the city (first element) of the last nominatim entry, or None if empty.\n",
    "    \"\"\"\n",
    "    if nom_list and len(nom_list) > 0:\n",
    "        return nom_list[-1][0]\n",
    "    return None\n",
    "\n",
    "# Add the new columns to did_centroids_ar.\n",
    "did_centroids_ar['after'] = did_centroids_ar['nominatim'].apply(extract_after)\n",
    "did_centroids_ar['origin'] = did_centroids_ar['nominatim'].apply(extract_origin)\n",
    "did_centroids_ar['final'] = did_centroids_ar['nominatim'].apply(extract_final)\n",
    "\n",
    "print(did_centroids_ar[['did', 'after', 'origin', 'final']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3add0c-c3c9-4dfe-9304-af5b3e80662b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "did_centroids_ar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ad55c6-3781-497e-a36b-81a8f6b19e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count rows where origin != final and final is 'Stuttgart'\n",
    "count = did_centroids_ar[(did_centroids_ar['origin'] != did_centroids_ar['final']) & (did_centroids_ar['final'] == 'Stuttgart')].shape[0]\n",
    "print(\"Count of rows with origin different from final and final 'Stuttgart':\", count/len(did_centroids_ar))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e24be180-fe58-45d6-a91d-88dd1cc21cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count rows where origin != final and final is 'Stuttgart'\n",
    "count = did_centroids_ar[(did_centroids_ar['origin'] != did_centroids_ar['final'])].shape[0]\n",
    "print(\"Count of rows with origin different from final:\", count/len(did_centroids_ar))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff2232e9-def8-4547-b238-576547909410",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import itertools\n",
    "\n",
    "# Flatten the \"after\" column from did_centroids_ar into one list of tuples.\n",
    "# Each row in did_centroids_ar['after'] is a list of tuples.\n",
    "all_after = list(itertools.chain.from_iterable(did_centroids_ar['after']))\n",
    "\n",
    "# Check that we have tuples and they are of the expected length.\n",
    "if all_after and len(all_after[0]) >= 4:\n",
    "    # Create counters for each tuple element.\n",
    "    counter_city    = Counter([t[0] for t in all_after])\n",
    "    counter_type    = Counter([t[1] for t in all_after])\n",
    "    counter_subtype = Counter([t[2] for t in all_after])\n",
    "    counter_id      = Counter([t[3] for t in all_after])\n",
    "else:\n",
    "    print(\"The 'after' column does not contain tuples of the expected length.\")\n",
    "    counter_city = counter_type = counter_subtype = counter_id = Counter()\n",
    "\n",
    "# Plot histograms for each tuple element in a 2x2 grid.\n",
    "fig, axs = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Histogram for the first element (city)\n",
    "axs[0, 0].bar(counter_city.keys(), counter_city.values())\n",
    "axs[0, 0].set_title(\"Histogram of City (First Entry)\")\n",
    "axs[0, 0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Histogram for the second element (type)\n",
    "axs[0, 1].bar(counter_type.keys(), counter_type.values())\n",
    "axs[0, 1].set_title(\"Histogram of Second Entry (Type)\")\n",
    "axs[0, 1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Histogram for the third element (sub-type)\n",
    "axs[1, 0].bar(counter_subtype.keys(), counter_subtype.values())\n",
    "axs[1, 0].set_title(\"Histogram of Third Entry (Sub-type)\")\n",
    "axs[1, 0].tick_params(axis='x', rotation=90)\n",
    "\n",
    "# Histogram for the fourth element (id)\n",
    "axs[1, 1].bar(counter_id.keys(), counter_id.values())\n",
    "axs[1, 1].set_title(\"Histogram of Fourth Entry (ID)\")\n",
    "axs[1, 1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38778cbd-ea23-41ed-8666-0fd86a77ea7e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
